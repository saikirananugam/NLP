{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "592_Assignment_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqXa53pSKHcviAXIdQVxub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saikirananugam/NLP/blob/main/592_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.\tFind stem and lemma words for the given words?"
      ],
      "metadata": {
        "id": "rRXKAq-sg8hJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL3pVqEvgw6J",
        "outputId": "c1fe5b82-21fc-4952-bdbd-dd1fbd548e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "from textblob import TextBlob, Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "stemmer = LancasterStemmer()\n",
        "words = [\"cats\",\"trouble\",\"troubling\",\"troubled\",\"having\",\"Corriendo\",\"at\",\"was\"]\n",
        "for word in words:\n",
        "  rootword = stemmer.stem(word)\n",
        "  print(rootword)"
      ],
      "metadata": {
        "id": "D0tJz2Khg7bL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c560d9c7-3d95-4814-ab9e-7c550a633d8b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "hav\n",
            "corriendo\n",
            "at\n",
            "was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization\n",
        "for word in words:\n",
        "  w = Word(word)\n",
        "  root = w.lemmatize()\n",
        "  print(root)"
      ],
      "metadata": {
        "id": "nPFYOripiAXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64dbca6-b822-4f30-f4d6-c89512b0c18d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "trouble\n",
            "troubling\n",
            "troubled\n",
            "having\n",
            "Corriendo\n",
            "at\n",
            "wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. find stop words and Bow from the given paragraph?"
      ],
      "metadata": {
        "id": "-caMP4XY50Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import chardet\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "mJqDfP4q5jWt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "paragraph = \"The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\"\n",
        "count_vec = CountVectorizer()\n",
        "count_occurs = count_vec.fit_transform([paragraph])\n",
        "count_occur_df = pd.DataFrame((count, word) for word, count in zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\n",
        "count_occur_df.columns = ['WORD','FREQUENCY']\n",
        "print(count_occur_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WfME8oc5Uyo",
        "outputId": "65c1884b-6a36-423c-a728-5fb2cc6536cf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          WORD  FREQUENCY\n",
            "0          and          3\n",
            "1           by          1\n",
            "2          can          2\n",
            "3     commonly          1\n",
            "4       corpus          1\n",
            "5       divide          1\n",
            "6        exits          1\n",
            "7         find          1\n",
            "8          for          1\n",
            "9         from          1\n",
            "10          if          1\n",
            "11          in          2\n",
            "12        into          1\n",
            "13          is          1\n",
            "14          it          1\n",
            "15    language          1\n",
            "16   libraries          1\n",
            "17     library          1\n",
            "18        list          2\n",
            "19      module          1\n",
            "20        most          1\n",
            "21     natural          1\n",
            "22        nltk          3\n",
            "23          of          3\n",
            "24      oldest          1\n",
            "25         one          1\n",
            "26  processing          1\n",
            "27    provided          1\n",
            "28      python          1\n",
            "29     removal          1\n",
            "30      remove          2\n",
            "31    sentence          1\n",
            "32        stop          4\n",
            "33    supports          1\n",
            "34        text          1\n",
            "35         the          6\n",
            "36        then          1\n",
            "37          to          1\n",
            "38        used          1\n",
            "39        word          2\n",
            "40       words          4\n",
            "41         you          2\n",
            "42        your          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Find BoW for the given paragraph? And also find stem and lemma words?"
      ],
      "metadata": {
        "id": "V9FHLWqQjdIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import chardet\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "fX5udRXJp5DX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "paragraph = \"Text Summarization is one of those applications of Natural Language Processing (NLP) which is bound to have a huge impact on our lives. With growing digital media and ever-growing publishing – who has the time to go through entire articles / documents / books to decide whether they are useful or not? Thankfully – this technology is already here.\"\n",
        "count_vec = CountVectorizer()\n",
        "count_occurs = count_vec.fit_transform([paragraph])\n",
        "count_occur_df = pd.DataFrame((count, word) for word, count in zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\n",
        "count_occur_df.columns = ['WORD','FREQUENCY']\n",
        "print(count_occur_df)"
      ],
      "metadata": {
        "id": "bZ93vVD25fUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75dfe7fc-a786-4e40-db2c-225ad748b22d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             WORD  FREQUENCY\n",
            "0         already          1\n",
            "1             and          1\n",
            "2    applications          1\n",
            "3             are          1\n",
            "4        articles          1\n",
            "5           books          1\n",
            "6           bound          1\n",
            "7          decide          1\n",
            "8         digital          1\n",
            "9       documents          1\n",
            "10         entire          1\n",
            "11           ever          1\n",
            "12             go          1\n",
            "13        growing          2\n",
            "14            has          1\n",
            "15           have          1\n",
            "16           here          1\n",
            "17           huge          1\n",
            "18         impact          1\n",
            "19             is          3\n",
            "20       language          1\n",
            "21          lives          1\n",
            "22          media          1\n",
            "23        natural          1\n",
            "24            nlp          1\n",
            "25            not          1\n",
            "26             of          2\n",
            "27             on          1\n",
            "28            one          1\n",
            "29             or          1\n",
            "30            our          1\n",
            "31     processing          1\n",
            "32     publishing          1\n",
            "33  summarization          1\n",
            "34     technology          1\n",
            "35           text          1\n",
            "36     thankfully          1\n",
            "37            the          1\n",
            "38           they          1\n",
            "39           this          1\n",
            "40          those          1\n",
            "41        through          1\n",
            "42           time          1\n",
            "43             to          3\n",
            "44         useful          1\n",
            "45        whether          1\n",
            "46          which          1\n",
            "47            who          1\n",
            "48           with          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "Ux_lpyp1rPr-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "GHT_hhw5sJ35"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stem words in paragraph\n",
        "text = word_tokenize(paragraph)\n",
        "for word in text:\n",
        "  stemword = stemmer.stem(word)\n",
        "  print(stemword)"
      ],
      "metadata": {
        "id": "XZ_5WDDKD9G1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba51220-77e3-406b-b883-b5fd73b0b1a7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text\n",
            "summar\n",
            "is\n",
            "one\n",
            "of\n",
            "those\n",
            "applic\n",
            "of\n",
            "natur\n",
            "languag\n",
            "process\n",
            "(\n",
            "nlp\n",
            ")\n",
            "which\n",
            "is\n",
            "bound\n",
            "to\n",
            "have\n",
            "a\n",
            "huge\n",
            "impact\n",
            "on\n",
            "our\n",
            "live\n",
            ".\n",
            "with\n",
            "grow\n",
            "digit\n",
            "media\n",
            "and\n",
            "ever-grow\n",
            "publish\n",
            "–\n",
            "who\n",
            "ha\n",
            "the\n",
            "time\n",
            "to\n",
            "go\n",
            "through\n",
            "entir\n",
            "articl\n",
            "/\n",
            "document\n",
            "/\n",
            "book\n",
            "to\n",
            "decid\n",
            "whether\n",
            "they\n",
            "are\n",
            "use\n",
            "or\n",
            "not\n",
            "?\n",
            "thank\n",
            "–\n",
            "thi\n",
            "technolog\n",
            "is\n",
            "alreadi\n",
            "here\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemma words in paragraph\n",
        "for word in text:\n",
        "  w = Word(word)\n",
        "  lemmaword = w.lemmatize()\n",
        "  print(lemmaword)"
      ],
      "metadata": {
        "id": "TRFIOhZDFmAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82ffee4-48fe-47f2-f205-5b7c63262570"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text\n",
            "Summarization\n",
            "is\n",
            "one\n",
            "of\n",
            "those\n",
            "application\n",
            "of\n",
            "Natural\n",
            "Language\n",
            "Processing\n",
            "(\n",
            "NLP\n",
            ")\n",
            "which\n",
            "is\n",
            "bound\n",
            "to\n",
            "have\n",
            "a\n",
            "huge\n",
            "impact\n",
            "on\n",
            "our\n",
            "life\n",
            ".\n",
            "With\n",
            "growing\n",
            "digital\n",
            "medium\n",
            "and\n",
            "ever-growing\n",
            "publishing\n",
            "–\n",
            "who\n",
            "ha\n",
            "the\n",
            "time\n",
            "to\n",
            "go\n",
            "through\n",
            "entire\n",
            "article\n",
            "/\n",
            "document\n",
            "/\n",
            "book\n",
            "to\n",
            "decide\n",
            "whether\n",
            "they\n",
            "are\n",
            "useful\n",
            "or\n",
            "not\n",
            "?\n",
            "Thankfully\n",
            "–\n",
            "this\n",
            "technology\n",
            "is\n",
            "already\n",
            "here\n",
            ".\n"
          ]
        }
      ]
    }
  ]
}